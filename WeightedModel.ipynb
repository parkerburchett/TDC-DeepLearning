{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test Best models and best weights on validation data .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbWmgF3-ssac",
        "outputId": "6b2dc7fc-6094-4d7a-b0cb-ca03e2d2211c"
      },
      "source": [
        "!git clone https://github.com/parkerburchett/pysmiles\n",
        "!pip install pyTDC\n",
        "!git clone https://github.com/parkerburchett/TDC-DeepLearning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pysmiles' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pyTDC in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyTDC) (0.22.2.post1)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (from pyTDC) (0.18.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pyTDC) (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pyTDC) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyTDC) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyTDC) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pyTDC) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyTDC) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pyTDC) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyTDC) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyTDC) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn->pyTDC) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->pyTDC) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->pyTDC) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->pyTDC) (2.4.7)\n",
            "fatal: destination path 'TDC-DeepLearning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bitBoDL0srnL",
        "outputId": "e9aa34ec-8696-4793-9150-da6a6d3b2581"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from pysmiles.pysmiles import read_smiles\n",
        "from tdc.single_pred import ADME\n",
        "import os\n",
        "os.chdir('/content/TDC-DeepLearning/')\n",
        "from utils import ColorRefinement as cr # this is the graph embedding Algo I wrote\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVlavl5JYDyq"
      },
      "source": [
        "def compute_auprc(y_true, y_pred):\n",
        "    # https://stats.stackexchange.com/questions/157012/area-under-precision-recall-curve-auc-of-pr-curve-and-average-precision-ap\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    area = round(auc(recall, precision), 6)\n",
        "    return area\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuh2Mg29sW6o",
        "outputId": "df4652a3-b525-45a9-e24b-c79b174a6c86"
      },
      "source": [
        "import json\n",
        "\n",
        "best_models_params = json.load(open('/content/drive/MyDrive/SpringBoard/Therapeutic Data Commons Projects/HyperParamTuning/bestmodels.json', 'r'))\n",
        "best_weights = pd.read_csv('/content/drive/MyDrive/SpringBoard/Therapeutic Data Commons Projects/HyperParamTuning/best_4_regression_weights.csv', index_col=0)\n",
        "\n",
        "\n",
        "best_models_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': \"{'colsample_bytree': 0.45921506474872353, 'learning_rate': 0.003605978989205916, 'n_estimators': 997, 'num_leaves': 171, 'reg_alpha': 0.06136193030050688, 'subsample': 0.664374000848817}\",\n",
              " '1': \"{'colsample_bytree': 0.19190373976042552, 'learning_rate': 0.018880733945270692, 'n_estimators': 772, 'num_leaves': 412, 'reg_alpha': 0.1319602189105627, 'subsample': 0.953435263598222}\",\n",
              " '2': \"{'colsample_bytree': 0.0846115062976256, 'learning_rate': 0.061904626017968235, 'n_estimators': 710, 'num_leaves': 218, 'reg_alpha': 0.09722107305351997, 'subsample': 0.7625401046034898}\",\n",
              " '3': \"{'colsample_bytree': 0.10234165146414909, 'learning_rate': 0.021876318417714605, 'n_estimators': 574, 'num_leaves': 193, 'reg_alpha': 0.08093256266597965, 'subsample': 0.9986613307559011}\",\n",
              " '4': \"{'colsample_bytree': 0.29036206035748535, 'learning_rate': 0.09316958191707549, 'n_estimators': 473, 'num_leaves': 101, 'reg_alpha': 0.1992406954388929, 'subsample': 0.005705227464680718}\"}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnY8peKxcFBt",
        "outputId": "850d9c46-78ac-4c49-f172-7ab1c7b99cd1"
      },
      "source": [
        "tuned_weights = best_weights.values[:-1].flatten()\n",
        "tuned_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.23494501, 0.50427143, 0.16934516, 0.0914384 ])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhab29-oYwXa",
        "outputId": "1163b787-8528-48ab-ddbc-bca8016169c1"
      },
      "source": [
        "model_params = [json.loads(best_models_params[i].replace(\"'\",'\"')) for i in best_models_params.keys()]\n",
        "tuned_models = [lgb.LGBMRegressor(subsample_freq=1, **p) for p in model_params[:-1]]\n",
        "tuned_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
              "               colsample_bytree=0.45921506474872353, importance_type='split',\n",
              "               learning_rate=0.003605978989205916, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=997, n_jobs=-1, num_leaves=171, objective=None,\n",
              "               random_state=None, reg_alpha=0.06136193030050688, reg_lambda=0.0,\n",
              "               silent=True, subsample=0.664374000848817,\n",
              "               subsample_for_bin=200000, subsample_freq=1),\n",
              " LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
              "               colsample_bytree=0.19190373976042552, importance_type='split',\n",
              "               learning_rate=0.018880733945270692, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=772, n_jobs=-1, num_leaves=412, objective=None,\n",
              "               random_state=None, reg_alpha=0.1319602189105627, reg_lambda=0.0,\n",
              "               silent=True, subsample=0.953435263598222,\n",
              "               subsample_for_bin=200000, subsample_freq=1),\n",
              " LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
              "               colsample_bytree=0.0846115062976256, importance_type='split',\n",
              "               learning_rate=0.061904626017968235, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=710, n_jobs=-1, num_leaves=218, objective=None,\n",
              "               random_state=None, reg_alpha=0.09722107305351997, reg_lambda=0.0,\n",
              "               silent=True, subsample=0.7625401046034898,\n",
              "               subsample_for_bin=200000, subsample_freq=1),\n",
              " LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
              "               colsample_bytree=0.10234165146414909, importance_type='split',\n",
              "               learning_rate=0.021876318417714605, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=574, n_jobs=-1, num_leaves=193, objective=None,\n",
              "               random_state=None, reg_alpha=0.08093256266597965, reg_lambda=0.0,\n",
              "               silent=True, subsample=0.9986613307559011,\n",
              "               subsample_for_bin=200000, subsample_freq=1)]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j9hsKxguSJ0"
      },
      "source": [
        "class WeightedModel:\n",
        "\n",
        "  def __init__(self,\n",
        "               cr_num_hops:int,\n",
        "               cr_num_colors:int, \n",
        "               models:list,\n",
        "               model_weights:list):\n",
        "    self.cr_num_hops = cr_num_hops\n",
        "    self.cr_num_colors = cr_num_colors\n",
        "    self.models = models\n",
        "    self.model_weights = model_weights\n",
        "\n",
        "\n",
        "  def faster_fit(self, model_df_pairs):\n",
        "    \"\"\"\n",
        "      Fit from model_df_pairs\n",
        "    \"\"\"\n",
        "    hop_feature_dfs = [pair['df'] for pair in model_df_pairs]\n",
        "    self.models = [pair['model_object'] for pair in model_df_pairs]  \n",
        "    for hop_num, model in enumerate(self.models):\n",
        "      df = hop_feature_dfs[hop_num]\n",
        "      X=df[self.features]\n",
        "      y=df[self.target]\n",
        "      model.fit(X,y)\n",
        "\n",
        "\n",
        "  def _create_embeddings(self, smiles):\n",
        "    graphs = [read_smiles(s, silent=True) for s in smiles]\n",
        "    hop_feature_dfs = cr.create_hop_feature_dfs(graphs=graphs,\n",
        "                                                num_hops=self.cr_num_hops,\n",
        "                                                num_colors=self.cr_num_colors)\n",
        "    print('embedded with Color Refinement')\n",
        "    return hop_feature_dfs\n",
        "\n",
        "\n",
        "  def fit(self, smiles, targets):\n",
        "    hop_feature_dfs = self._create_embeddings(smiles)\n",
        "    for hop_num, model in enumerate(self.models):\n",
        "      X = hop_feature_dfs[hop_num].values\n",
        "      y = targets.values\n",
        "      model.fit(X,y)\n",
        "      print('fit a model')\n",
        "\n",
        "\n",
        "  def predict(self, smiles):\n",
        "    hop_feature_dfs = self._create_embeddings(smiles)\n",
        "    prediction_df = pd.DataFrame()\n",
        "    for hop_num, model in enumerate(self.models):\n",
        "      prediction_df[hop_num] = model.predict(hop_feature_dfs[hop_num].values)\n",
        "    weighted_predictions = prediction_df.values.dot(self.model_weights)\n",
        "    return weighted_predictions\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eofdWEncn6R",
        "outputId": "64425417-3751-4403-e23b-1e26c42f7863"
      },
      "source": [
        "from tdc.benchmark_group import admet_group\n",
        "group = admet_group(path = 'data/')\n",
        "benchmark = group.get('cyp2c9_veith')\n",
        "\n",
        "valid_df = benchmark['test'][['Drug', 'Y']]\n",
        "train_df = benchmark['train_val'][['Drug', 'Y']]\n",
        "name = benchmark['name']\n",
        "predictions = {}\n",
        "\n",
        "final_model = WeightedModel(cr_num_hops=4,\n",
        "                            cr_num_colors=10_000,\n",
        "                            models=tuned_models,\n",
        "                            model_weights=tuned_weights)\n",
        "\n",
        "train_smiles, train_targets = train_df['Drug'], train_df['Y']\n",
        "final_model.fit(train_smiles, train_targets)\n",
        "preds = final_model.predict(valid_df['Drug'])\n",
        "\n",
        "selfCalcuated_auprc = compute_auprc(valid_df['Y'], preds)\n",
        "print('you calcuated auprc at ', selfCalcuated_auprc)\n",
        "\n",
        "predictions[name] = preds \n",
        "print(group.evaluate(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found local copy...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedded with Color Refinement\n",
            "fit a model\n",
            "fit a model\n",
            "fit a model\n",
            "fit a model\n",
            "embedded with Color Refinement\n",
            "you calcuated auprc at  0.770984\n",
            "{'cyp2c9_veith': {'pr-auc': 0.771}}\n"
          ]
        }
      ]
    }
  ]
}